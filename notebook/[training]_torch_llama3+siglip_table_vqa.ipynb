{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_lwLs3JZvwR"
      },
      "source": [
        "# langauge+vision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0J5lbxNjHWO"
      },
      "source": [
        "## import library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7wFkgx6UoFf",
        "outputId": "d9749f9e-30cb-42d4-a2d9-65fe034cb2af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.21.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.12.0)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.43.3)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.32.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.4.0+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.42.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.66.5)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.23.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2024.6.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.19.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: trl in /usr/local/lib/python3.10/dist-packages (0.10.1)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from trl) (2.4.0+cu121)\n",
            "Requirement already satisfied: transformers>=4.31.0 in /usr/local/lib/python3.10/dist-packages (from trl) (4.42.4)\n",
            "Requirement already satisfied: numpy>=1.18.2 in /usr/local/lib/python3.10/dist-packages (from trl) (1.26.4)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (from trl) (0.32.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from trl) (2.21.0)\n",
            "Requirement already satisfied: tyro>=0.5.11 in /usr/local/lib/python3.10/dist-packages (from trl) (0.8.10)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (2024.6.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->trl) (0.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->trl) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->trl) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->trl) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->trl) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->trl) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->trl) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->trl) (4.66.5)\n",
            "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl) (0.16)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl) (13.8.0)\n",
            "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl) (1.7.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->trl) (5.9.5)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (2.1.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (3.10.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->trl) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->trl) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->trl) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->trl) (2024.7.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.16.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.4.0->trl) (2.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->trl) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->trl) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->trl) (2024.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.4.0->trl) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->trl) (1.16.0)\n",
            "Requirement already satisfied: lightning in /usr/local/lib/python3.10/dist-packages (2.4.0)\n",
            "Requirement already satisfied: PyYAML<8.0,>=5.4 in /usr/local/lib/python3.10/dist-packages (from lightning) (6.0.2)\n",
            "Requirement already satisfied: fsspec<2026.0,>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (2024.6.1)\n",
            "Requirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (0.11.6)\n",
            "Requirement already satisfied: packaging<25.0,>=20.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (24.1)\n",
            "Requirement already satisfied: torch<4.0,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (2.4.0+cu121)\n",
            "Requirement already satisfied: torchmetrics<3.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (1.4.1)\n",
            "Requirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<6.0,>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.12.2)\n",
            "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.10/dist-packages (from lightning) (2.4.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (3.10.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities<2.0,>=0.10.0->lightning) (71.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.15.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.1.4)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics<3.0,>=0.7.0->lightning) (1.26.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<4.0,>=2.1.0->lightning) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<4.0,>=2.1.0->lightning) (1.3.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (3.8)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n"
          ]
        }
      ],
      "source": [
        "! pip install datasets\n",
        "! pip install peft bitsandbytes accelerate\n",
        "! pip install trl\n",
        "! pip install lightning\n",
        "! pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSQYq3-m_41p",
        "outputId": "b0f0432f-e291-477a-fc34-5f6b9284cced"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRANid3DJZc8"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from PIL import Image\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    LlamaForCausalLM,\n",
        "    SiglipImageProcessor,\n",
        "    SiglipVisionModel,\n",
        "    AutoProcessor,\n",
        "    TrainingArguments,\n",
        "    LlavaForConditionalGeneration,\n",
        ")\n",
        "from transformers import TextStreamer\n",
        "from peft import get_peft_model, LoraConfig\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "from transformers.data.data_collator import DataCollatorForLanguageModeling\n",
        "import lightning as L\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import re\n",
        "from nltk import edit_distance\n",
        "import numpy as np\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from io import BytesIO\n",
        "import requests\n",
        "import zipfile\n",
        "import os\n",
        "from trl import SFTTrainer, SFTConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44Ioulzjim7X"
      },
      "outputs": [],
      "source": [
        "config = {\"max_epochs\": 2,\n",
        "          \"val_check_interval\": 0.5, # how many times we want to validate during an epoch\n",
        "          \"check_val_every_n_epoch\": 1,\n",
        "          \"gradient_clip_val\": 1.0,\n",
        "          \"accumulate_grad_batches\": 8,\n",
        "          \"lr\": 1e-5,\n",
        "          \"batch_size\": 2,\n",
        "          # \"seed\":2022,\n",
        "          \"num_nodes\": 1,\n",
        "          \"warmup_steps\": 50,\n",
        "          \"result_path\": \"./result\",\n",
        "          \"verbose\": True,\n",
        "          \"max_length\": 1024,\n",
        "          \"except_image_max_length\": 512,\n",
        "          \"model_name\": \"unsloth/llama-3-8b-Instruct\",\n",
        "          \"vision_model_name\": \"google/siglip-so400m-patch14-384\",\n",
        "          \"model_embedding_size\": 4096,\n",
        "          \"vision_model_embedding_size\": 1152,\n",
        "}\n",
        "\n",
        "# \"MLP-KTLim/llama-3-Korean-Bllossom-8B\"\n",
        "# \"unsloth/llama-3-8b-Instruct\"\n",
        "# \"microsoft/Phi-3-mini-4k-instruct\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XaW07_KJT55"
      },
      "outputs": [],
      "source": [
        "# !git clone https://huggingface.co/qresearch/llama-3-vision-alpha"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqcpNc2S9nnY"
      },
      "source": [
        "## model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7ZWVAwzJZiC"
      },
      "outputs": [],
      "source": [
        "def initialize_models():\n",
        "\n",
        "    llm = config.get(\"model_name\")\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        # bnb_4bit_compute_type=torch.bfloat16\n",
        "    )\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        llm, use_fast=True\n",
        "    )\n",
        "    tokenizer.padding_side = \"right\"\n",
        "    model = LlamaForCausalLM.from_pretrained(\n",
        "        llm,\n",
        "        # torch_dtype=torch.float16,\n",
        "        device_map=\"auto\",\n",
        "        quantization_config=bnb_config,\n",
        "        attn_implementation=\"eager\",\n",
        "        output_hidden_states = True,\n",
        "    )\n",
        "\n",
        "    for param in model.base_model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    model_name = config.get(\"vision_model_name\")\n",
        "    # model_name = \"\"\n",
        "    vision_model = SiglipVisionModel.from_pretrained(\n",
        "        model_name,\n",
        "        # torch_dtype=torch.float16\n",
        "    )\n",
        "    processor = SiglipImageProcessor.from_pretrained(model_name)\n",
        "\n",
        "    vision_model = vision_model.to(\"cuda\")\n",
        "\n",
        "    return tokenizer, model, vision_model, processor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cqnOGVS1JZkg"
      },
      "outputs": [],
      "source": [
        "class ProjectionModule(nn.Module):\n",
        "    def __init__(self, mm_hidden_size, hidden_size):\n",
        "        super(ProjectionModule, self).__init__()\n",
        "\n",
        "        # Directly set up the sequential model\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(mm_hidden_size, hidden_size),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_fn0eH-_JkK4"
      },
      "outputs": [],
      "source": [
        "# def load_projection_module(mm_hidden_size=1152, hidden_size=4096, device=\"cuda\"):\n",
        "#     projection_module = ProjectionModule(mm_hidden_size, hidden_size)\n",
        "#     checkpoint = torch.load(\"./mm_projector.bin\")\n",
        "#     # checkpoint = state_dict\n",
        "#     checkpoint = {k.replace(\"mm_projector.\", \"\"): v for k, v in checkpoint.items()}\n",
        "#     projection_module.load_state_dict(checkpoint)\n",
        "#     projection_module = projection_module.to(device).half()\n",
        "#     return projection_module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "aac9a11fa570402589ed2cd1368b7282"
          ]
        },
        "id": "MyzvAi_FagVI",
        "outputId": "1f18de3e-a9f2-4e41-cb6f-0925d379c6e7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aac9a11fa570402589ed2cd1368b7282",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "tokenizer, model, vision_model, processor = initialize_models()\n",
        "tokenizer.eos_token = \"<|eot_id|>\"\n",
        "\n",
        "projection_module = ProjectionModule(mm_hidden_size=config.get(\"vision_model_embedding_size\"), hidden_size=config.get(\"model_embedding_size\")) #4096, 3072\n",
        "projection_module = projection_module.to(\"cuda\") #.half()\n",
        "# state_dict = new_dict()\n",
        "# projection_module = load_projection_module()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oU0jXRti0mvq",
        "outputId": "64c55251-4f49-4754-9adc-2a400b1cc829"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424\n"
          ]
        }
      ],
      "source": [
        "lora_config = LoraConfig(\n",
        "    r=4,\n",
        "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\"],\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIrM6vkY0m7G",
        "outputId": "0b8619ed-4147-4bb3-9b11-2dc184994d48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 373,248 || all params: 428,598,848 || trainable%: 0.0871\n"
          ]
        }
      ],
      "source": [
        "lora_config = LoraConfig(\n",
        "    r=2,\n",
        "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\"],\n",
        "    # task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "vision_model = get_peft_model(vision_model, lora_config)\n",
        "vision_model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NsMV6Gi9slU"
      },
      "source": [
        "## dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5J0wBiY2jNu"
      },
      "outputs": [],
      "source": [
        "# initial_prompt = {\n",
        "#         \"role\": \"system\",\n",
        "#         \"content\": \"Present a chat with an assistant and a user with a lot of questions. The assistant understands the intent and purpose of the question and answers it accurately. <image>\",\n",
        "#     }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_mij-eMZGqD"
      },
      "outputs": [],
      "source": [
        "# def tokenizer_image_token(prompt, tokenizer,max_length=config.get(\"except_image_max_length\"),  image_token_index= 500000):\n",
        "\n",
        "#     prompt_chunks = prompt.split(\"<image>\")\n",
        "#     tokenized_chunks = [tokenizer(chunk, truncation = True, padding = True,max_length=max_length).input_ids for chunk in prompt_chunks]\n",
        "#     input_ids = tokenized_chunks[0]\n",
        "\n",
        "#     for chunk in tokenized_chunks[1:]:\n",
        "#         input_ids.append(image_token_index)\n",
        "#         input_ids.extend(chunk[1:])  # Exclude BOS token on nonzero index\n",
        "\n",
        "#     return torch.tensor(input_ids, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FafEnCtxW1Qd"
      },
      "outputs": [],
      "source": [
        "# # for cmarkea/table-vqa\n",
        "# class CustomDataset(Dataset):\n",
        "#     def __init__(self,dataset, tokenizer, processor,split = \"train\" ,max_length=2048):\n",
        "#         super().__init__()\n",
        "#         self.tokenizer = tokenizer\n",
        "#         self.processor = processor\n",
        "#         self.max_length = max_length\n",
        "#         self.split = split\n",
        "#         self.dataset = dataset\n",
        "#         self.dataset_length = len(self.dataset)\n",
        "\n",
        "#     def __len__(self) -> int:\n",
        "#         return self.dataset_length\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "\n",
        "#         batch = self.dataset[idx]\n",
        "#         texts = [x for x in batch['qa']['en']]\n",
        "#         imgs = batch['image']\n",
        "\n",
        "#         conversation = []\n",
        "#         conversation.append(initial_prompt)\n",
        "#         for _, qa in enumerate(texts):\n",
        "#             conversation.append({\"role\": \"user\", \"content\": qa[\"question\"]})\n",
        "#             conversation.append({\"role\": \"assistant\", \"content\": qa[\"answer\"]})\n",
        "#         text = self.tokenizer.apply_chat_template(conversation,tokenize=False)\n",
        "\n",
        "#         input_id = tokenizer_image_token(text, self.tokenizer, max_length=self.max_length).unsqueeze(0)\n",
        "\n",
        "#         image = imgs.convert(\"RGB\")\n",
        "#         image_inputs = self.processor(\n",
        "#             images=image, # [image],\n",
        "#             return_tensors=\"pt\",\n",
        "#             do_resize=True,\n",
        "#             size={\"height\": 384, \"width\": 384},\n",
        "#         )\n",
        "#         pixel_values = image_inputs[\"pixel_values\"]\n",
        "\n",
        "#         result = {'input_ids': input_id, 'pixel_values': pixel_values , 'texts': text}\n",
        "#         return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXSk3PorOiAQ"
      },
      "outputs": [],
      "source": [
        "# # for cmarkea/table-vqa\n",
        "# class CustomDataset2(Dataset):\n",
        "#     def __init__(self,dataset, tokenizer, processor,split = \"train\" ,max_length=2048):\n",
        "#         super().__init__()\n",
        "#         self.tokenizer = tokenizer\n",
        "#         self.processor = processor\n",
        "#         self.max_length = max_length\n",
        "#         self.split = split\n",
        "#         self.dataset = dataset\n",
        "#         self.dataset_length = len(self.dataset)\n",
        "\n",
        "\n",
        "#     def __len__(self) -> int:\n",
        "#         return self.dataset_length\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "\n",
        "#         batch = self.dataset[idx]\n",
        "#         texts = [x for x in batch['qa']['en']]\n",
        "#         imgs = batch['image']\n",
        "\n",
        "#         questions = []\n",
        "#         answers = []\n",
        "\n",
        "#         for _, qa in enumerate(texts):\n",
        "#             question = []\n",
        "#             question.append(initial_prompt)\n",
        "#             question.append({\"role\": \"user\", \"content\": qa[\"question\"]})\n",
        "#             questions.append(question)\n",
        "#             answers.append(qa[\"answer\"])\n",
        "#         text = self.tokenizer.apply_chat_template(questions,tokenize=False)\n",
        "\n",
        "\n",
        "#         new_text = []\n",
        "#         for t in text:\n",
        "#           prompt_chunks = t.split(\"<image>\")\n",
        "#           tokenized_chunks = [tokenizer(chunk, truncation = True, padding = True,max_length=self.max_length).input_ids for chunk in prompt_chunks]\n",
        "#           input_ids = tokenized_chunks[0]\n",
        "\n",
        "#           for chunk in tokenized_chunks[1:]:\n",
        "#               input_ids.append(-200)\n",
        "#               input_ids.extend(chunk[1:])  # Exclude BOS token on nonzero index\n",
        "\n",
        "#           input_id =  torch.tensor(input_ids, dtype=torch.long)\n",
        "\n",
        "#           new_text.append(input_id)\n",
        "\n",
        "#         input_id = torch.nn.utils.rnn.pad_sequence(new_text, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n",
        "\n",
        "\n",
        "#         image = imgs.convert(\"RGB\")\n",
        "#         image_inputs = self.processor(\n",
        "#             images=image, # [image],\n",
        "#             return_tensors=\"pt\",\n",
        "#             do_resize=True,\n",
        "#             size={\"height\": 384, \"width\": 384},\n",
        "#         )\n",
        "#         pixel_values = image_inputs[\"pixel_values\"]\n",
        "\n",
        "#         new_pix = []\n",
        "#         for _ in range(len(text)):\n",
        "#           new_pix.append(pixel_values)\n",
        "\n",
        "#         pixel_values = torch.cat(new_pix, dim = 0)\n",
        "\n",
        "#         result = {'input_ids': input_id, 'pixel_values': pixel_values , 'answers': answers,  'texts': text}\n",
        "#         return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hSwoAgHARYsE"
      },
      "outputs": [],
      "source": [
        "# # for cmarkea/table-vqa\n",
        "# class DataCollatorForCustomVLM(DataCollatorForLanguageModeling):\n",
        "#     def __init__(self, tokenizer, mlm=False):\n",
        "#         super().__init__(tokenizer, mlm)\n",
        "\n",
        "#     def __call__(self, batch):\n",
        "#         input_ids = [item['input_ids'].squeeze(0) for item in batch]\n",
        "#         pixel_values = [item['pixel_values'] for item in batch]\n",
        "\n",
        "#         input_ids_padded = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n",
        "\n",
        "#         pixel_values = torch.cat(pixel_values, dim = 0)\n",
        "\n",
        "#         labels = input_ids_padded.clone()\n",
        "#         if self.tokenizer.pad_token_id is not None:\n",
        "#             labels[labels == self.tokenizer.pad_token_id] = -100\n",
        "\n",
        "#         return {\n",
        "#             'input_ids': input_ids_padded,\n",
        "#             'pixel_values': pixel_values,\n",
        "#             'labels': labels\n",
        "#         }\n",
        "\n",
        "# data_collator = DataCollatorForCustomVLM(tokenizer=tokenizer, mlm=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tsHtNFQH3al1"
      },
      "outputs": [],
      "source": [
        "# # for cmarkea/table-vqa\n",
        "# class DataCollatorForCustomVLM2:\n",
        "#     def __init__(self):\n",
        "#       pass\n",
        "\n",
        "#     def __call__(self, batch):\n",
        "\n",
        "#         return batch\n",
        "\n",
        "# data_collator2 = DataCollatorForCustomVLM2()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czKfgXcW2awm"
      },
      "outputs": [],
      "source": [
        "# # raw_datasets = load_dataset(\"liuhaotian/LLaVA-Instruct-150K\")\n",
        "# raw_datasets = load_dataset(\"cmarkea/table-vqa\")\n",
        "# train = raw_datasets[\"train\"]\n",
        "# valid = raw_datasets[\"test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmROfuuYo9BZ"
      },
      "outputs": [],
      "source": [
        "# train_dataset = CustomDataset(train, tokenizer, processor,split = \"train\" ,max_length=config.get(\"max_length\"))\n",
        "# val_dataset = CustomDataset2(valid, tokenizer, processor,split = \"test\" ,max_length=config.get(\"max_length\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IbaF-XHw1DNO"
      },
      "outputs": [],
      "source": [
        "# train[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttAs8U5A2NgU"
      },
      "outputs": [],
      "source": [
        "# print(train_dataset[0]['texts'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lb_43UOH4DGr"
      },
      "source": [
        "## dataset2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thmjyGiBCmOo"
      },
      "outputs": [],
      "source": [
        "def tokenizer_image_token(prompt, tokenizer,max_length=config.get(\"except_image_max_length\"),  image_token_index= 500000):\n",
        "\n",
        "    prompt_chunks = prompt.split(\"<image>\")\n",
        "    tokenized_chunks = [tokenizer(chunk, truncation = True, padding = True,max_length=max_length).input_ids for chunk in prompt_chunks]\n",
        "    input_ids = tokenized_chunks[0]\n",
        "\n",
        "    for chunk in tokenized_chunks[1:]:\n",
        "        input_ids.append(image_token_index)\n",
        "        input_ids.extend(chunk[1:])  # Exclude BOS token on nonzero index\n",
        "\n",
        "    attention_mask = torch.ones(len(input_ids), dtype=torch.long)\n",
        "\n",
        "    return torch.tensor(input_ids, dtype=torch.long), attention_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VtGWtHJdAB0D"
      },
      "outputs": [],
      "source": [
        "LLAVA_CHAT_TEMPLATE = \"\"\"{% for message in messages %} \\\n",
        "  {% if message['from'] == 'human' %}\n",
        "    USER: {{ message['value'] }} \\\n",
        "  {% else %}\n",
        "    ASSISTANT: {{ message['value'] }} \\\n",
        "  {% endif %} \\\n",
        "  {% if message['from'] == 'gpt' %} \\\n",
        "  {% else %} \\\n",
        "      {{ eos_token }} \\\n",
        "  {% endif %} \\\n",
        "{% endfor %}\"\"\"\n",
        "\n",
        "tokenizer.chat_template = LLAVA_CHAT_TEMPLATE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ok-CesuQ4ENL"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self,dataset,tokenizer, processor,split = \"train\" ,max_length=2048):\n",
        "        super().__init__()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.processor = processor\n",
        "        self.max_length = max_length\n",
        "        self.split = split\n",
        "        self.dataset = dataset\n",
        "        self.dataset_length = len(self.dataset)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return self.dataset_length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        batch = self.dataset[idx]\n",
        "        texts = batch['conversations']\n",
        "        text = self.tokenizer.apply_chat_template(texts,tokenize=False)\n",
        "        input_id = tokenizer_image_token(text, self.tokenizer, max_length=self.max_length)[0].unsqueeze(0)\n",
        "        attention_mask = tokenizer_image_token(text, self.tokenizer, max_length=self.max_length)[1].unsqueeze(0)\n",
        "\n",
        "        img_name = batch['image']\n",
        "        imgs = os.path.join(\"/content/extracted_images/\", img_name)\n",
        "        imgs = Image.open(imgs)\n",
        "        image = imgs.convert(\"RGB\")\n",
        "        image_inputs = self.processor(\n",
        "            images=image, # [image],\n",
        "            return_tensors=\"pt\",\n",
        "            do_resize=True,\n",
        "            size={\"height\": 384, \"width\": 384},\n",
        "        )\n",
        "        pixel_values = image_inputs[\"pixel_values\"]\n",
        "\n",
        "\n",
        "        result = {'input_ids': input_id, 'attention_mask' : attention_mask ,'pixel_values': pixel_values , 'texts': text}\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZhBjsU14EU2"
      },
      "outputs": [],
      "source": [
        "class CustomDataset2(Dataset):\n",
        "    def __init__(self,dataset,tokenizer, processor,split = \"train\" ,max_length=2048):\n",
        "        super().__init__()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.processor = processor\n",
        "        self.max_length = max_length\n",
        "        self.split = split\n",
        "        self.dataset = dataset\n",
        "        self.dataset_length = len(self.dataset)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return self.dataset_length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        batch = self.dataset[idx]\n",
        "        texts = batch['conversations']\n",
        "        text = self.tokenizer.apply_chat_template(texts,tokenize=False)\n",
        "        question = text.split(\"<|eot_id|>\")[0]\n",
        "        question += \" ASSISTANT:\"\n",
        "        answer = text.split(\"<|eot_id|>\")[1].replace(\"ASSISTANT:\", \"\")\n",
        "\n",
        "        input_id = tokenizer_image_token(question, self.tokenizer, max_length=self.max_length)[0].unsqueeze(0)\n",
        "        attention_mask = tokenizer_image_token(question, self.tokenizer, max_length=self.max_length)[1].unsqueeze(0)\n",
        "        img_name = batch['image']\n",
        "\n",
        "        imgs = os.path.join(\"/content/extracted_images/\", img_name)\n",
        "        imgs = Image.open(imgs)\n",
        "        image = imgs.convert(\"RGB\")\n",
        "        image_inputs = self.processor(\n",
        "            images=image, # [image],\n",
        "            return_tensors=\"pt\",\n",
        "            do_resize=True,\n",
        "            size={\"height\": 384, \"width\": 384},\n",
        "        )\n",
        "        pixel_values = image_inputs[\"pixel_values\"]\n",
        "\n",
        "        result = {'input_ids': input_id, 'attention_mask' : attention_mask, 'pixel_values': pixel_values , 'questions': question, 'answers': answer }\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yu74ihvm4EXW"
      },
      "outputs": [],
      "source": [
        "# raw_datasets = load_dataset(\"lmms-lab/LLaVA-OneVision-Data\", 'TabMWP(MathV360K)')\n",
        "raw_datasets = load_dataset(\"liuhaotian/LLaVA-CC3M-Pretrain-595K\",data_files = \"chat.json\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CisWpK7GNN7",
        "outputId": "638d225b-734a-42e0-8f68-17c1eea6c79a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'LLaVA-CC3M-Pretrain-595K' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://huggingface.co/datasets/liuhaotian/LLaVA-CC3M-Pretrain-595K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWo5BE41GeMa"
      },
      "outputs": [],
      "source": [
        "zip_file_path = \"/content/LLaVA-CC3M-Pretrain-595K/images.zip\"\n",
        "#    \n",
        "extract_to = \"extracted_images\"\n",
        "\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_to)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rjcvJiKG4EaF"
      },
      "outputs": [],
      "source": [
        "train = raw_datasets[\"train\"]\n",
        "\n",
        "train_valid_split = train.train_test_split(test_size=0.2, shuffle=True, seed=42)\n",
        "\n",
        "# Access the train and validation sets\n",
        "train_split = train_valid_split['train']\n",
        "valid_split = train_valid_split['test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3R2imYGX4Ecu",
        "outputId": "c52358d2-b9aa-4688-e8c5-f752a600778f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'GCC_train_002140770.jpg'"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_split[0]['image']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXG6Knd84EfQ"
      },
      "outputs": [],
      "source": [
        "train_dataset = CustomDataset(train_split,tokenizer, processor,split = \"train\" ,max_length=config.get(\"max_length\"))\n",
        "val_dataset = CustomDataset2(valid_split,tokenizer, processor,split = \"train\" ,max_length=config.get(\"max_length\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGDWszYB_Uok",
        "outputId": "66b13a25-48a0-4465-843c-761b8e864768"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 33])\n"
          ]
        }
      ],
      "source": [
        "print(train_dataset[1]['attention_mask'].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4AQynicG_Uq-"
      },
      "outputs": [],
      "source": [
        "class DataCollatorForCustomVLM(DataCollatorForLanguageModeling):\n",
        "    def __init__(self, tokenizer, mlm=False):\n",
        "        super().__init__(tokenizer, mlm)\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        input_ids = [item['input_ids'].squeeze(0) for item in batch]\n",
        "        attention_mask = [item['attention_mask'].squeeze(0) for item in batch]\n",
        "        pixel_values = [item['pixel_values'] for item in batch]\n",
        "\n",
        "        input_ids_padded = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n",
        "        attention_mask = torch.nn.utils.rnn.pad_sequence(attention_mask, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n",
        "\n",
        "        pixel_values = torch.cat(pixel_values, dim = 0)\n",
        "\n",
        "        labels = input_ids_padded.clone()\n",
        "        if self.tokenizer.pad_token_id is not None:\n",
        "            labels[labels == self.tokenizer.pad_token_id] = -100\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids_padded,\n",
        "            'attention_mask':attention_mask,\n",
        "            'pixel_values': pixel_values,\n",
        "            'labels': labels\n",
        "        }\n",
        "\n",
        "class DataCollatorForCustomVLM2(DataCollatorForLanguageModeling):\n",
        "    def __init__(self, tokenizer, mlm=False):\n",
        "        super().__init__(tokenizer, mlm)\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        input_ids = [item['input_ids'].squeeze(0) for item in batch]\n",
        "        attention_mask = [item['attention_mask'].squeeze(0) for item in batch]\n",
        "        pixel_values = [item['pixel_values'] for item in batch]\n",
        "        answers = [item['answers'] for item in batch]\n",
        "\n",
        "        input_ids_padded = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n",
        "        attention_mask = torch.nn.utils.rnn.pad_sequence(attention_mask, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n",
        "\n",
        "        pixel_values = torch.cat(pixel_values, dim = 0)\n",
        "\n",
        "        labels = input_ids_padded.clone()\n",
        "        if self.tokenizer.pad_token_id is not None:\n",
        "            labels[labels == self.tokenizer.pad_token_id] = -100\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids_padded,\n",
        "            'attention_mask':attention_mask,\n",
        "            'pixel_values': pixel_values,\n",
        "            'labels': labels,\n",
        "            'answers': answers\n",
        "        }\n",
        "data_collator = DataCollatorForCustomVLM(tokenizer=tokenizer, mlm=False)\n",
        "data_collator2 = DataCollatorForCustomVLM2(tokenizer=tokenizer, mlm=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYlCeoZPlvLI"
      },
      "source": [
        " ## custom vision langauge model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_p8lknBAJZfc"
      },
      "outputs": [],
      "source": [
        "def process_tensors(input_ids, attention_mask, image_features, embedding_layer):\n",
        "\n",
        "    total_ids = []\n",
        "    total_attn = []\n",
        "\n",
        "    for i in range(input_ids.shape[0]):\n",
        "\n",
        "      input = input_ids[i].unsqueeze(0)\n",
        "      attention = attention_mask[i].unsqueeze(0)\n",
        "      image = image_features[i].unsqueeze(0)\n",
        "\n",
        "      if not isinstance(input, torch.Tensor):\n",
        "          input = torch.tensor(input)\n",
        "\n",
        "      split_index = (input == 500000).nonzero(as_tuple=True)[1]\n",
        "\n",
        "      input_ids_1 = input[:, :split_index]\n",
        "      input_ids_2 = input[:, split_index + 1 :]\n",
        "\n",
        "      # Convert input_ids to embeddings\n",
        "      embeddings_1 = embedding_layer(input_ids_1)\n",
        "      embeddings_2 = embedding_layer(input_ids_2)\n",
        "\n",
        "      device = image.device\n",
        "      token_embeddings_part1 = embeddings_1.to(device)\n",
        "      token_embeddings_part2 = embeddings_2.to(device)\n",
        "\n",
        "      # Concatenate the token embeddings and image features\n",
        "      concatenated_embedding = torch.cat(\n",
        "          [token_embeddings_part1, image, token_embeddings_part2], dim=1\n",
        "      )\n",
        "\n",
        "      attention_mask_1 = attention[:, :split_index]\n",
        "      attention_mask_2 = attention[:, split_index + 1 :]\n",
        "      image = torch.ones(\n",
        "          image.shape[:2], dtype=torch.long\n",
        "      )\n",
        "      idevice = image.device\n",
        "      cat_attention_mask = torch.cat(\n",
        "          [attention_mask_1.to(idevice), image, attention_mask_2.to(idevice)], dim=1\n",
        "      )\n",
        "\n",
        "      # # Create the corrected attention mask\n",
        "      # attention_mask = torch.ones(\n",
        "      #     concatenated_embedding.shape[:2], dtype=torch.long\n",
        "      # )\n",
        "\n",
        "      total_ids.append(concatenated_embedding)\n",
        "\n",
        "      total_attn.append(cat_attention_mask)\n",
        "\n",
        "    concatenated_embeddings = torch.cat(total_ids, dim=0)\n",
        "    cat_attention_masks = torch.cat(total_attn, dim=0)\n",
        "    return concatenated_embeddings , cat_attention_masks\n",
        "\n",
        "def process_labels(input_ids, image_features):\n",
        "\n",
        "    total_embed = []\n",
        "\n",
        "    for i in range(input_ids.shape[0]):\n",
        "      input = input_ids[i].unsqueeze(0)\n",
        "      image = image_features[i].unsqueeze(0)\n",
        "\n",
        "      if not isinstance(input, torch.Tensor):\n",
        "          input = torch.tensor(input)\n",
        "\n",
        "      split_index = (input == 500000).nonzero(as_tuple=True)[1][0]\n",
        "\n",
        "      input_ids_1 = input[:, :split_index]\n",
        "      input_ids_2 = input[:, split_index + 1 :]\n",
        "\n",
        "      device = image.device\n",
        "      pbatch = image.shape[0]\n",
        "      pseq = image.shape[1]\n",
        "      image_token = torch.full([pbatch,pseq,], -100, dtype=torch.long).to(device)\n",
        "\n",
        "      # Concatenate the token embeddings and image features\n",
        "      concatenated_embedding = torch.cat(\n",
        "          [input_ids_1, image_token, input_ids_2], dim=1\n",
        "      )\n",
        "      total_embed.append(concatenated_embedding)\n",
        "\n",
        "    concatenated_embeddings = torch.cat(total_embed, dim=0)\n",
        "    return concatenated_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8BbToNc3WIE"
      },
      "outputs": [],
      "source": [
        "class custom_vlm(L.LightningModule):\n",
        "    def __init__(self, config, vision_model, model, projection_module, tokenizer):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.model = model\n",
        "        self.vision_model = vision_model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.projection_module = projection_module\n",
        "        self.batch_size = config.get(\"batch_size\")\n",
        "        self.loss_fn = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "        # self.automatic_optimization = False #  \n",
        "\n",
        "\n",
        "    def on_train_start(self):\n",
        "        self.model.train()\n",
        "        self.vision_model.train()\n",
        "        self.projection_module.train()\n",
        "\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        self.model.train()\n",
        "        self.vision_model.train()\n",
        "        self.projection_module.train()\n",
        "        # opt = self.optimizers() # automatic_optimization F\n",
        "\n",
        "\n",
        "        input_ids = batch['input_ids'].to(self.device) # value long\n",
        "        pixel_values = batch['pixel_values'].to(self.device)\n",
        "        attention_mask = batch['attention_mask']\n",
        "\n",
        "        # with autocast():\n",
        "        image_forward_outs = self.vision_model(\n",
        "            pixel_values.to(device=self.device,dtype=torch.float16), #.unsqueeze(0),\n",
        "            output_hidden_states=True,\n",
        "        ) # value float16\n",
        "\n",
        "        image_features = image_forward_outs.hidden_states[-2]\n",
        "        projected_embeddings = self.projection_module(image_features).to(self.device) # module float32 + value float16\n",
        "\n",
        "        embedding_layer = self.model.get_input_embeddings()\n",
        "\n",
        "        #   \n",
        "        new_embeds , attn_mask = process_tensors(\n",
        "            input_ids, attention_mask, projected_embeddings, embedding_layer\n",
        "        ) # value float16\n",
        "\n",
        "        labels = process_labels(\n",
        "            input_ids, projected_embeddings\n",
        "        ) # value float16\n",
        "\n",
        "        attn_mask = attn_mask.to(self.device)\n",
        "        new_embeds = new_embeds.to(self.device)\n",
        "        labels = labels.to(self.device)\n",
        "\n",
        "        outputs = self.model(inputs_embeds=new_embeds, attention_mask=attn_mask) # module float32 + value float16\n",
        "        logits = outputs.logits\n",
        "\n",
        "\n",
        "        logits = logits.view(-1, logits.size(-1))  # Shape: [batch_size * sequence_length, vocab_size]\n",
        "        labels = labels.view(-1)  # Shape: [batch_size * sequence_length]\n",
        "\n",
        "\n",
        "        loss = self.loss_fn(logits, labels)\n",
        "        # opt.zero_grad() # automatic_optimization F\n",
        "        # self.manual_backward(loss) # automatic_optimization F\n",
        "        # opt.step() # automatic_optimization F\n",
        "\n",
        "        self.log(\"train_loss\", loss, on_step=True, on_epoch=False, prog_bar=True, logger=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx, dataset_idx=0):\n",
        "        self.model.eval()\n",
        "        self.vision_model.eval()\n",
        "        self.projection_module.eval()\n",
        "\n",
        "\n",
        "        answers = batch['answers']\n",
        "        input_ids = batch['input_ids'].to(self.device) # value long\n",
        "        pixel_values = batch['pixel_values'].to(self.device)\n",
        "        attention_mask = batch['attention_mask']\n",
        "\n",
        "        # answers = batch[0]['answers']\n",
        "        # input_ids = batch[0]['input_ids'] # value long\n",
        "        # pixel_values = batch[0]['pixel_values']\n",
        "\n",
        "        # with autocast():\n",
        "        image_forward_outs = self.vision_model(\n",
        "            pixel_values.to(device=self.device,dtype=torch.float16), #.unsqueeze(0),\n",
        "            output_hidden_states=True,\n",
        "        ) # value float16\n",
        "\n",
        "        image_features = image_forward_outs.hidden_states[-2]\n",
        "        projected_embeddings = self.projection_module(image_features).to(self.device) # module float32 + value float16\n",
        "\n",
        "        embedding_layer = self.model.get_input_embeddings()\n",
        "\n",
        "        #   \n",
        "        new_embeds, attn_mask = process_tensors(\n",
        "            input_ids, attention_mask, projected_embeddings, embedding_layer\n",
        "        ) # value float16\n",
        "\n",
        "        attn_mask = attn_mask.to(self.device)\n",
        "        new_embeds = new_embeds.to(self.device)\n",
        "\n",
        "        # autoregressively generate token IDs\n",
        "        generated_ids = self.model.generate(inputs_embeds=new_embeds.to(dtype=torch.float16), attention_mask=attn_mask, max_new_tokens=128)\n",
        "        # turn them back into text, chopping of the prompt\n",
        "        # important: we don't skip special tokens here, because we want to see them in the output\n",
        "        predictions = self.tokenizer.batch_decode(generated_ids[:, input_ids.size(1):], skip_special_tokens=True)\n",
        "\n",
        "        scores = []\n",
        "        for pred, answer in zip(predictions, answers):\n",
        "            pred = re.sub(r\"(?:(?<=>) | (?=</s_))\", \"\", pred)\n",
        "            scores.append(edit_distance(pred, answer) / max(len(pred), len(answer)))\n",
        "\n",
        "            if self.config.get(\"verbose\", False) and len(scores) == 1:\n",
        "                print(f\"Prediction: {pred}\")\n",
        "                print(f\"    Answer: {answer}\")\n",
        "                print(f\" Normed ED: {scores[0]}\")\n",
        "\n",
        "        self.log(\"val_edit_distance\", np.mean(scores), on_step=True, on_epoch=False, prog_bar=True, logger=True)\n",
        "\n",
        "        return scores\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        # you could also add a learning rate scheduler if you want\n",
        "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.config.get(\"lr\"))\n",
        "\n",
        "        return optimizer\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(train_dataset, collate_fn=data_collator, batch_size=self.batch_size, shuffle=True, num_workers=4)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(val_dataset, collate_fn=data_collator2, batch_size=self.batch_size, shuffle=True, num_workers=4)\n",
        "    # def val_dataloader(self):\n",
        "    #     return DataLoader(val_dataset, collate_fn=data_collator2, batch_size=1, shuffle=False, num_workers=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfuC5S48X_mu"
      },
      "outputs": [],
      "source": [
        "model_module = custom_vlm(config, vision_model, model, projection_module, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7yWMdTAlvUh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpbU31TB9yry"
      },
      "source": [
        "## training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEXshRlmHI9q",
        "outputId": "f6469682-acbe-4809-f274-8e70ff1824ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding(128256, 4096, padding_idx=128255)\n",
            "Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n"
          ]
        }
      ],
      "source": [
        "print(model.get_input_embeddings())\n",
        "print(vision_model.get_input_embeddings())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2SZ4q-4kPuW",
        "outputId": "7e0f0e64-5f05-4d5b-b8ac-cb5eb49d5b00"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "21504000"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sum(p.numel() for p in projection_module.parameters() if p.requires_grad == True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17qEbgj1HoEL",
        "outputId": "2b8daa24-4af9-4cf2-b2b7-94abc7078697"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total trainable parameters: 25285120\n"
          ]
        }
      ],
      "source": [
        "total_params = sum(p.numel() for p in model_module.parameters() if p.requires_grad == True)\n",
        "print(f\"Total trainable parameters: {total_params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9WaCF3jAzRW"
      },
      "outputs": [],
      "source": [
        "for name, module in projection_module.named_modules():\n",
        "    if isinstance(module,torch.nn.Linear):\n",
        "        module.weight.requires_grad = False\n",
        "        module.bias.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "00c33153cc204b65b8c02e4afd29ef22",
            "9409562fc76643a793bee60733b6bd4d",
            "ac738f5c4afc481792d43f8cf03c22b1",
            "733ba221c6b8404aa7c5c2130d8a91ab",
            "dd71a55c80284e4a990803308bd9d199",
            "3a12a3b59bf54b029e06d0636640c195",
            "c9530fb121c64b53adf918fcfc1536e0",
            "22107ffbfff74c2ba3d7a5646f7bb030",
            "9c97968a955f4c96b378cd71d2c1a981",
            "a1a46bfccb0342749ff400cf4a27eff0",
            "af91d16b2ec248a3a4f01ba40ce59353",
            "0c309c2260bb4a9b968ecc9a75aaa26f",
            "e21b981711dd48b78585910993555024",
            "4fe95c1e39f54a3faebd63bd48b88cc6",
            "903e7defcd994b0fbb175d55a2ea0abd",
            "502ab0ec6f2b47259abe2b51c4762128",
            "06deb926d8c24fd68ae8151555732773",
            "bc6326eba14f4b7cb9272e6bb01c4e39",
            "d7b42425577045c18106f78965122370",
            "c3e2baeb64904ab8a9f55d8e0716d5eb",
            "a88b062b30ed46e593d046136c6cc7b0",
            "94ee2628b11b4b21a3a421d8bf3b9208",
            "af95e84672734c7bbdf7b99d0e2e04e3",
            "197ceb3069634d108e2373d6ead2c27b",
            "c67add08d6b7491a8c0863218a0b861b",
            "90a3d8c43de34d4097c3ab933925af94",
            "7aa7280af98a413d9de2bbcc4b7a8689",
            "1a511570332d40fe9f2558f6d02d511d",
            "40531b020533449ba8eb5533b1025cf4",
            "1f8e8c194fb74a30acdc4604d22ff8f5",
            "3788356e345241ee9c6a56cd441eb161",
            "89bd3040bf61421f8389758d9259410b",
            "4eda4de0ffb4481899f5e68f6e4708d2",
            "5b9960ae5fba4aa7b0ede4a6efb7de9a",
            "70510c2d0991472e96e8c28ac3759d09"
          ]
        },
        "id": "lM9MeTLA-m9e",
        "outputId": "05a19986-48c6-4e01-c3ea-dcfd2a595d5a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO: Using 16bit Automatic Mixed Precision (AMP)\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Using 16bit Automatic Mixed Precision (AMP)\n",
            "INFO: GPU available: True (cuda), used: True\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO: \n",
            "  | Name              | Type                 | Params | Mode \n",
            "-------------------------------------------------------------------\n",
            "0 | model             | PeftModelForCausalLM | 4.5 B  | train\n",
            "1 | vision_model      | PeftModel            | 428 M  | train\n",
            "2 | projection_module | ProjectionModule     | 21.5 M | train\n",
            "3 | loss_fn           | CrossEntropyLoss     | 0      | train\n",
            "-------------------------------------------------------------------\n",
            "3.8 M     Trainable params\n",
            "5.0 B     Non-trainable params\n",
            "5.0 B     Total params\n",
            "19,976.444Total estimated model params size (MB)\n",
            "2100      Modules in train mode\n",
            "794       Modules in eval mode\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name              | Type                 | Params | Mode \n",
            "-------------------------------------------------------------------\n",
            "0 | model             | PeftModelForCausalLM | 4.5 B  | train\n",
            "1 | vision_model      | PeftModel            | 428 M  | train\n",
            "2 | projection_module | ProjectionModule     | 21.5 M | train\n",
            "3 | loss_fn           | CrossEntropyLoss     | 0      | train\n",
            "-------------------------------------------------------------------\n",
            "3.8 M     Trainable params\n",
            "5.0 B     Non-trainable params\n",
            "5.0 B     Total params\n",
            "19,976.444Total estimated model params size (MB)\n",
            "2100      Modules in train mode\n",
            "794       Modules in eval mode\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "00c33153cc204b65b8c02e4afd29ef22",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/nn/modules.py:435: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
            "  warnings.warn(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0c309c2260bb4a9b968ecc9a75aaa26f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction: :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
            "    Answer:             a man cooking up dishes at a stall at the night food market in capital          \n",
            " Normed ED: 1.0\n",
            "Prediction:                                                                                                         \n",
            "    Answer:             cute baby elephant walks towards the camera          \n",
            " Normed ED: 0.7307692307692307\n",
            "Prediction: :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
            "    Answer:             property image # - beautiful new log cabin nestled in person          \n",
            " Normed ED: 1.0\n",
            "Prediction: :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
            "    Answer:             a student gets sleepy during state school .          \n",
            " Normed ED: 1.0\n",
            "Prediction:                                                                                                         \n",
            "    Answer:             battles for the ball with athlete during the teams final regular - season game of the season .          \n",
            " Normed ED: 0.6637931034482759\n",
            "Prediction: :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
            "    Answer:             an aerial view during its redevelopment          \n",
            " Normed ED: 1.0\n",
            "Prediction:                                                                                                            \n",
            "    Answer:             the broken old fishing pier on the lake          \n",
            " Normed ED: 0.7289719626168224\n",
            "Prediction: ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
            "    Answer:             interior of the custom screen door -- extra large size          \n",
            " Normed ED: 1.0\n",
            "Prediction:                                                                                                          \n",
            "    Answer:             view across the beach with its funfair and ferris wheel on a bright sunny day          \n",
            " Normed ED: 0.6571428571428571\n",
            "Prediction:                                                                                                         \n",
            "    Answer:             a group of export paintings          \n",
            " Normed ED: 0.75\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e21b981711dd48b78585910993555024",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction: :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
            "    Answer:             a man cooking up dishes at a stall at the night food market in capital          \n",
            " Normed ED: 1.0\n",
            "Prediction:                                                                                                         \n",
            "    Answer:             cute baby elephant walks towards the camera          \n",
            " Normed ED: 0.7307692307692307\n",
            "Prediction: :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
            "    Answer:             property image # - beautiful new log cabin nestled in person          \n",
            " Normed ED: 1.0\n",
            "Prediction: ::::::::::::::::::::::::::::::::::::::::::::::\n",
            "    Answer:             a student gets sleepy during state school .          \n",
            " Normed ED: 1.0\n",
            "Prediction:                                                                                                         \n",
            "    Answer:             battles for the ball with athlete during the teams final regular - season game of the season .          \n",
            " Normed ED: 0.6637931034482759\n",
            "Prediction: :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
            "    Answer:             an aerial view during its redevelopment          \n",
            " Normed ED: 1.0\n",
            "Prediction:  the the the the the the the    the        the the the the                                          the  the                     the the                   \n",
            "    Answer:             the broken old fishing pier on the lake          \n",
            " Normed ED: 0.7612903225806451\n",
            "Prediction: ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
            "    Answer:             interior of the custom screen door -- extra large size          \n",
            " Normed ED: 1.0\n",
            "Prediction:                                                                                                          \n",
            "    Answer:             view across the beach with its funfair and ferris wheel on a bright sunny day          \n",
            " Normed ED: 0.6571428571428571\n",
            "Prediction:                                                                                                         \n",
            "    Answer:             a group of export paintings          \n",
            " Normed ED: 0.75\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4fe95c1e39f54a3faebd63bd48b88cc6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction: :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
            "    Answer:             a man cooking up dishes at a stall at the night food market in capital          \n",
            " Normed ED: 1.0\n",
            "Prediction:                                                                                                         \n",
            "    Answer:             cute baby elephant walks towards the camera          \n",
            " Normed ED: 0.7307692307692307\n",
            "Prediction: :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
            "    Answer:             property image # - beautiful new log cabin nestled in person          \n",
            " Normed ED: 1.0\n",
            "Prediction: :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
            "    Answer:             a student gets sleepy during state school .          \n",
            " Normed ED: 1.0\n",
            "Prediction:                                                                                                         \n",
            "    Answer:             battles for the ball with athlete during the teams final regular - season game of the season .          \n",
            " Normed ED: 0.6637931034482759\n",
            "Prediction: :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
            "    Answer:             an aerial view during its redevelopment          \n",
            " Normed ED: 1.0\n",
            "Prediction:            the                                                                                                \n",
            "    Answer:             the broken old fishing pier on the lake          \n",
            " Normed ED: 0.7272727272727273\n",
            "Prediction: ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
            "    Answer:             interior of the custom screen door -- extra large size          \n",
            " Normed ED: 1.0\n",
            "Prediction:                                                                                                          \n",
            "    Answer:             view across the beach with its funfair and ferris wheel on a bright sunny day          \n",
            " Normed ED: 0.6571428571428571\n",
            "Prediction:                                                                                                         \n",
            "    Answer:             a group of export paintings          \n",
            " Normed ED: 0.75\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "903e7defcd994b0fbb175d55a2ea0abd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction: :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
            "    Answer:             a man cooking up dishes at a stall at the night food market in capital          \n",
            " Normed ED: 1.0\n",
            "Prediction:                                                                                                         \n",
            "    Answer:             cute baby elephant walks towards the camera          \n",
            " Normed ED: 0.7307692307692307\n",
            "Prediction: :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
            "    Answer:             property image # - beautiful new log cabin nestled in person          \n",
            " Normed ED: 1.0\n",
            "Prediction: :::::::::::::::::::::::::::::::::::::::::ldndaNldnda\n",
            "    Answer:             a student gets sleepy during state school .          \n",
            " Normed ED: 1.0\n",
            "Prediction:                                                                                                         \n",
            "    Answer:             battles for the ball with athlete during the teams final regular - season game of the season .          \n",
            " Normed ED: 0.6637931034482759\n",
            "Prediction: :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
            "    Answer:             an aerial view during its redevelopment          \n",
            " Normed ED: 1.0\n",
            "Prediction:            the  the  the                                                                                   the        the \n",
            "    Answer:             the broken old fishing pier on the lake          \n",
            " Normed ED: 0.7377049180327869\n",
            "Prediction: ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
            "    Answer:             interior of the custom screen door -- extra large size          \n",
            " Normed ED: 1.0\n",
            "Prediction:                                                 the                                                         \n",
            "    Answer:             view across the beach with its funfair and ferris wheel on a bright sunny day          \n",
            " Normed ED: 0.6574074074074074\n",
            "Prediction:                                                                                                         \n",
            "    Answer:             a group of export paintings          \n",
            " Normed ED: 0.75\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "502ab0ec6f2b47259abe2b51c4762128",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction: :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
            "    Answer:             a man cooking up dishes at a stall at the night food market in capital          \n",
            " Normed ED: 1.0\n",
            "Prediction:                                                                                                         \n",
            "    Answer:             cute baby elephant walks towards the camera          \n",
            " Normed ED: 0.7307692307692307\n",
            "Prediction: :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
            "    Answer:             property image # - beautiful new log cabin nestled in person          \n",
            " Normed ED: 1.0\n",
            "Prediction: :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
            "    Answer:             a student gets sleepy during state school .          \n",
            " Normed ED: 1.0\n",
            "Prediction:                                                                                                         \n",
            "    Answer:             battles for the ball with athlete during the teams final regular - season game of the season .          \n",
            " Normed ED: 0.6637931034482759\n",
            "Prediction: :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
            "    Answer:             an aerial view during its redevelopment          \n",
            " Normed ED: 1.0\n",
            "Prediction:            the                                                                                                \n",
            "    Answer:             the broken old fishing pier on the lake          \n",
            " Normed ED: 0.7272727272727273\n",
            "Prediction: ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
            "    Answer:             interior of the custom screen door -- extra large size          \n",
            " Normed ED: 1.0\n",
            "Prediction:                                                                                                          \n",
            "    Answer:             view across the beach with its funfair and ferris wheel on a bright sunny day          \n",
            " Normed ED: 0.6571428571428571\n",
            "Prediction:                                                                                                         \n",
            "    Answer:             a group of export paintings          \n",
            " Normed ED: 0.75\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "06deb926d8c24fd68ae8151555732773",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction: :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
            "    Answer:             a man cooking up dishes at a stall at the night food market in capital          \n",
            " Normed ED: 1.0\n",
            "Prediction:                                                                                                         \n",
            "    Answer:             cute baby elephant walks towards the camera          \n",
            " Normed ED: 0.7307692307692307\n",
            "Prediction: :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
            "    Answer:             property image # - beautiful new log cabin nestled in person          \n",
            " Normed ED: 1.0\n",
            "Prediction: :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
            "    Answer:             a student gets sleepy during state school .          \n",
            " Normed ED: 1.0\n",
            "Prediction:                                                                                                         \n",
            "    Answer:             battles for the ball with athlete during the teams final regular - season game of the season .          \n",
            " Normed ED: 0.6637931034482759\n",
            "Prediction: :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
            "    Answer:             an aerial view during its redevelopment          \n",
            " Normed ED: 1.0\n",
            "Prediction:    the  the the     the                                       the the                              the                  the the  the     \n",
            "    Answer:             the broken old fishing pier on the lake          \n",
            " Normed ED: 0.7445255474452555\n",
            "Prediction: ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
            "    Answer:             interior of the custom screen door -- extra large size          \n",
            " Normed ED: 1.0\n",
            "Prediction:                              the   the   the the  the the the the the the the the the the the the the the the the the the                                     the the   the         \n",
            "    Answer:             view across the beach with its funfair and ferris wheel on a bright sunny day          \n",
            " Normed ED: 0.7166666666666667\n",
            "Prediction:                                                                                                         \n",
            "    Answer:             a group of export paintings          \n",
            " Normed ED: 0.75\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bc6326eba14f4b7cb9272e6bb01c4e39",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction: :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
            "    Answer:             a man cooking up dishes at a stall at the night food market in capital          \n",
            " Normed ED: 1.0\n",
            "Prediction:                                                                                                         \n",
            "    Answer:             cute baby elephant walks towards the camera          \n",
            " Normed ED: 0.7307692307692307\n",
            "Prediction: :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
            "    Answer:             property image # - beautiful new log cabin nestled in person          \n",
            " Normed ED: 1.0\n",
            "Prediction: :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
            "    Answer:             a student gets sleepy during state school .          \n",
            " Normed ED: 1.0\n",
            "Prediction:                                                                                                         \n",
            "    Answer:             battles for the ball with athlete during the teams final regular - season game of the season .          \n",
            " Normed ED: 0.6637931034482759\n",
            "Prediction: :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
            "    Answer:             an aerial view during its redevelopment          \n",
            " Normed ED: 1.0\n",
            "Prediction:            the                                                                                                \n",
            "    Answer:             the broken old fishing pier on the lake          \n",
            " Normed ED: 0.7272727272727273\n",
            "Prediction: ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
            "    Answer:             interior of the custom screen door -- extra large size          \n",
            " Normed ED: 1.0\n",
            "Prediction:      the the the  the the       the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the                           the the the the the the the the the the the the the      \n",
            "    Answer:             view across the beach with its funfair and ferris wheel on a bright sunny day          \n",
            " Normed ED: 0.8247422680412371\n",
            "Prediction:                                                                                                         \n",
            "    Answer:             a group of export paintings          \n",
            " Normed ED: 0.75\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d7b42425577045c18106f78965122370",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction: :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
            "    Answer:             a man cooking up dishes at a stall at the night food market in capital          \n",
            " Normed ED: 1.0\n",
            "Prediction:                                                                                                         \n",
            "    Answer:             cute baby elephant walks towards the camera          \n",
            " Normed ED: 0.7307692307692307\n",
            "Prediction: :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
            "    Answer:             property image # - beautiful new log cabin nestled in person          \n",
            " Normed ED: 1.0\n",
            "Prediction: :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
            "    Answer:             a student gets sleepy during state school .          \n",
            " Normed ED: 1.0\n",
            "Prediction:                                                                                                         \n",
            "    Answer:             battles for the ball with athlete during the teams final regular - season game of the season .          \n",
            " Normed ED: 0.6637931034482759\n",
            "Prediction: :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
            "    Answer:             an aerial view during its redevelopment          \n",
            " Normed ED: 1.0\n",
            "Prediction:            the                                                                                        the        \n",
            "    Answer:             the broken old fishing pier on the lake          \n",
            " Normed ED: 0.7345132743362832\n",
            "Prediction: ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
            "    Answer:             interior of the custom screen door -- extra large size          \n",
            " Normed ED: 1.0\n",
            "Prediction:     the the the               the the the the the  the the the the the the the the the the the the the the the the the the the the the the the the the the  the the the         the    the the          the     the   the the the the the the the the the the the the  the    \n",
            "    Answer:             view across the beach with its funfair and ferris wheel on a bright sunny day          \n",
            " Normed ED: 0.8111111111111111\n",
            "Prediction:                                                                                                         \n",
            "    Answer:             a group of export paintings          \n",
            " Normed ED: 0.75\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c3e2baeb64904ab8a9f55d8e0716d5eb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction: :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
            "    Answer:             a man cooking up dishes at a stall at the night food market in capital          \n",
            " Normed ED: 1.0\n",
            "Prediction:                                                                                                         \n",
            "    Answer:             cute baby elephant walks towards the camera          \n",
            " Normed ED: 0.7307692307692307\n",
            "Prediction: :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
            "    Answer:             property image # - beautiful new log cabin nestled in person          \n",
            " Normed ED: 1.0\n",
            "Prediction: :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
            "    Answer:             a student gets sleepy during state school .          \n",
            " Normed ED: 1.0\n",
            "Prediction:                                                                                                         \n",
            "    Answer:             battles for the ball with athlete during the teams final regular - season game of the season .          \n",
            " Normed ED: 0.6637931034482759\n",
            "Prediction: :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
            "    Answer:             an aerial view during its redevelopment          \n",
            " Normed ED: 1.0\n",
            "Prediction:            the                                                                                                \n",
            "    Answer:             the broken old fishing pier on the lake          \n",
            " Normed ED: 0.7272727272727273\n",
            "Prediction: ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
            "    Answer:             interior of the custom screen door -- extra large size          \n",
            " Normed ED: 1.0\n",
            "Prediction:     the  the  the the the the the the the the the the the the  the the the    the the the the the the the the the the the  the the the the the the the the the the the the the the the  the                                the the the the the the the the the the the the the      the\n",
            "    Answer:             view across the beach with its funfair and ferris wheel on a bright sunny day          \n",
            " Normed ED: 0.8172043010752689\n",
            "Prediction:                              the                                                                           \n",
            "    Answer:             a group of export paintings          \n",
            " Normed ED: 0.7476635514018691\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a88b062b30ed46e593d046136c6cc7b0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction: :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
            "    Answer:             a man cooking up dishes at a stall at the night food market in capital          \n",
            " Normed ED: 1.0\n",
            "Prediction:                                                                                                         \n",
            "    Answer:             cute baby elephant walks towards the camera          \n",
            " Normed ED: 0.7307692307692307\n",
            "Prediction: :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
            "    Answer:             property image # - beautiful new log cabin nestled in person          \n",
            " Normed ED: 1.0\n",
            "Prediction: :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
            "    Answer:             a student gets sleepy during state school .          \n",
            " Normed ED: 1.0\n",
            "Prediction:                                                                                                         \n",
            "    Answer:             battles for the ball with athlete during the teams final regular - season game of the season .          \n",
            " Normed ED: 0.6637931034482759\n",
            "Prediction: :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
            "    Answer:             an aerial view during its redevelopment          \n",
            " Normed ED: 1.0\n",
            "Prediction:            the                                                                                                \n",
            "    Answer:             the broken old fishing pier on the lake          \n",
            " Normed ED: 0.7272727272727273\n",
            "Prediction: ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
            "    Answer:             interior of the custom screen door -- extra large size          \n",
            " Normed ED: 1.0\n",
            "Prediction:         the   the  the the the the the the the the the the  the the the the the the the the  the the the the the the the the the the the the the the the the the  the the   the the the the the  the the  the   the the the the the the     the   the      the     of the the the the the the the the  the  the  \n",
            "    Answer:             view across the beach with its funfair and ferris wheel on a bright sunny day          \n",
            " Normed ED: 0.8327868852459016\n",
            "Prediction:                                                                                                         \n",
            "    Answer:             a group of export paintings          \n",
            " Normed ED: 0.75\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "94ee2628b11b4b21a3a421d8bf3b9208",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction: :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
            "    Answer:             a man cooking up dishes at a stall at the night food market in capital          \n",
            " Normed ED: 1.0\n",
            "Prediction:                                                                                                         \n",
            "    Answer:             cute baby elephant walks towards the camera          \n",
            " Normed ED: 0.7307692307692307\n",
            "Prediction: :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
            "    Answer:             property image # - beautiful new log cabin nestled in person          \n",
            " Normed ED: 1.0\n",
            "Prediction: :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
            "    Answer:             a student gets sleepy during state school .          \n",
            " Normed ED: 1.0\n",
            "Prediction:                                                                                                         \n",
            "    Answer:             battles for the ball with athlete during the teams final regular - season game of the season .          \n",
            " Normed ED: 0.6637931034482759\n",
            "Prediction: :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
            "    Answer:             an aerial view during its redevelopment          \n",
            " Normed ED: 1.0\n",
            "Prediction:            the                                                                                                \n",
            "    Answer:             the broken old fishing pier on the lake          \n",
            " Normed ED: 0.7272727272727273\n",
            "Prediction: ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
            "    Answer:             interior of the custom screen door -- extra large size          \n",
            " Normed ED: 1.0\n",
            "Prediction:              the    the the   an an the the of the the the the    the     the  the  the the the the the the the the the the the the the the the the the  the the   the   the the  the     the the the the the the     the     the an the the the the the the the the the     \n",
            "    Answer:             view across the beach with its funfair and ferris wheel on a bright sunny day          \n",
            " Normed ED: 0.7992565055762082\n",
            "Prediction:                                                                                                         \n",
            "    Answer:             a group of export paintings          \n",
            " Normed ED: 0.75\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "af95e84672734c7bbdf7b99d0e2e04e3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction: :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
            "    Answer:             a man cooking up dishes at a stall at the night food market in capital          \n",
            " Normed ED: 1.0\n",
            "Prediction:                                                                                                         \n",
            "    Answer:             cute baby elephant walks towards the camera          \n",
            " Normed ED: 0.7307692307692307\n",
            "Prediction: :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
            "    Answer:             property image # - beautiful new log cabin nestled in person          \n",
            " Normed ED: 1.0\n",
            "Prediction: :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
            "    Answer:             a student gets sleepy during state school .          \n",
            " Normed ED: 1.0\n",
            "Prediction:                                                                                                         \n",
            "    Answer:             battles for the ball with athlete during the teams final regular - season game of the season .          \n",
            " Normed ED: 0.6637931034482759\n",
            "Prediction: :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
            "    Answer:             an aerial view during its redevelopment          \n",
            " Normed ED: 1.0\n",
            "Prediction:                                                                                                            \n",
            "    Answer:             the broken old fishing pier on the lake          \n",
            " Normed ED: 0.7289719626168224\n",
            "Prediction: ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
            "    Answer:             interior of the custom screen door -- extra large size          \n",
            " Normed ED: 1.0\n",
            "Prediction:    the the the the the the the the  the the the the the the the the the the the the the the the the the the the the the the the the the the the the  the the the the the the the the the the the the the the the the the the the  the     the the     the  the the the the the the the   the  the  the  the the  the of the the the  the the the the  the  \n",
            "    Answer:             view across the beach with its funfair and ferris wheel on a bright sunny day          \n",
            " Normed ED: 0.8530259365994236\n",
            "Prediction:                                                                                                         \n",
            "    Answer:             a group of export paintings          \n",
            " Normed ED: 0.75\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "197ceb3069634d108e2373d6ead2c27b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction: :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
            "    Answer:             a man cooking up dishes at a stall at the night food market in capital          \n",
            " Normed ED: 1.0\n",
            "Prediction:                                                                                                         \n",
            "    Answer:             cute baby elephant walks towards the camera          \n",
            " Normed ED: 0.7307692307692307\n",
            "Prediction: :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
            "    Answer:             property image # - beautiful new log cabin nestled in person          \n",
            " Normed ED: 1.0\n",
            "Prediction: :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
            "    Answer:             a student gets sleepy during state school .          \n",
            " Normed ED: 1.0\n",
            "Prediction:                                                                                                         \n",
            "    Answer:             battles for the ball with athlete during the teams final regular - season game of the season .          \n",
            " Normed ED: 0.6637931034482759\n",
            "Prediction: :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
            "    Answer:             an aerial view during its redevelopment          \n",
            " Normed ED: 1.0\n",
            "Prediction:                                                                                                            \n",
            "    Answer:             the broken old fishing pier on the lake          \n",
            " Normed ED: 0.7289719626168224\n",
            "Prediction: ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
            "    Answer:             interior of the custom screen door -- extra large size          \n",
            " Normed ED: 1.0\n",
            "Prediction:  the the the for the the  of the of the the the the the the the the the the the the the the the the the the the the the  of the the the the the the the the the the the the the the the the the the the the the the the the the the the  the the the  the   the the  the the the the the the the the the the the   the the the the the the the the the the the the the the the the the the   the\n",
            "    Answer:             view across the beach with its funfair and ferris wheel on a bright sunny day          \n",
            " Normed ED: 0.8645833333333334\n",
            "Prediction:                                                                                                         \n",
            "    Answer:             a group of export paintings          \n",
            " Normed ED: 0.75\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c67add08d6b7491a8c0863218a0b861b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction: :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
            "    Answer:             a man cooking up dishes at a stall at the night food market in capital          \n",
            " Normed ED: 1.0\n",
            "Prediction:                                                                                                         \n",
            "    Answer:             cute baby elephant walks towards the camera          \n",
            " Normed ED: 0.7307692307692307\n",
            "Prediction: :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
            "    Answer:             property image # - beautiful new log cabin nestled in person          \n",
            " Normed ED: 1.0\n",
            "Prediction: :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
            "    Answer:             a student gets sleepy during state school .          \n",
            " Normed ED: 1.0\n",
            "Prediction:                                                                                                         \n",
            "    Answer:             battles for the ball with athlete during the teams final regular - season game of the season .          \n",
            " Normed ED: 0.6637931034482759\n",
            "Prediction: :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
            "    Answer:             an aerial view during its redevelopment          \n",
            " Normed ED: 1.0\n",
            "Prediction:            the                                                    the                                 the the          \n",
            "    Answer:             the broken old fishing pier on the lake          \n",
            " Normed ED: 0.7058823529411765\n",
            "Prediction: ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
            "    Answer:             interior of the custom screen door -- extra large size          \n",
            " Normed ED: 1.0\n",
            "Prediction:      the the the the the the the the the the the the the the the an the the  the the the the the the the the the of the the the the the the the an the the the the the the the the the the the the the the the the an the the the the the an  the the the the the the the the the the the the the the the the the the an the the the the the the the of the the the the the the the the the the the the the\n",
            "    Answer:             view across the beach with its funfair and ferris wheel on a bright sunny day          \n",
            " Normed ED: 0.8531645569620253\n",
            "Prediction:                               the                                                                          \n",
            "    Answer:             a group of export paintings          \n",
            " Normed ED: 0.7476635514018691\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO: \n",
            "Detected KeyboardInterrupt, attempting graceful shutdown ...\n",
            "INFO:lightning.pytorch.utilities.rank_zero:\n",
            "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'exit' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\u001b[0m in \u001b[0;36m_fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    573\u001b[0m         )\n\u001b[0;32m--> 574\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    980\u001b[0m         \u001b[0;31m# ----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\u001b[0m in \u001b[0;36m_run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1024\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_detect_anomaly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_detect_anomaly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1025\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1026\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py\u001b[0m in \u001b[0;36madvance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_fetcher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_fetcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_fetcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_fetcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\u001b[0m in \u001b[0;36madvance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    249\u001b[0m                     \u001b[0;31m# in automatic optimization, there can only be one optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m                     \u001b[0mbatch_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautomatic_optimization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, optimizer, batch_idx, kwargs)\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_block_parallel_sync_behavior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m                 \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py\u001b[0m in \u001b[0;36mclosure\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backward_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mstep_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosure_loss\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosure_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py\u001b[0m in \u001b[0;36mbackward_fn\u001b[0;34m(loss)\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mbackward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m             \u001b[0mcall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_strategy_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"backward\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py\u001b[0m in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[Strategy]{trainer.strategy.__class__.__name__}.{hook_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/strategy.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, closure_loss, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecision_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/plugins/precision/precision.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, tensor, model, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \"\"\"\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/core/module.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1101\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    520\u001b[0m             )\n\u001b[0;32m--> 521\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    290\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    769\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-27a33da7ad23>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    536\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainerStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRUNNING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 538\u001b[0;31m         call._call_and_handle_interrupt(\n\u001b[0m\u001b[1;32m    539\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_SubprocessScriptLauncher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mlauncher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_sigkill_signal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'exit' is not defined"
          ]
        }
      ],
      "source": [
        "from lightning.pytorch.loggers import WandbLogger\n",
        "\n",
        "# wandb_logger = WandbLogger(project=WANDB_PROJECT, name=WANDB_NAME)\n",
        "\n",
        "trainer = L.Trainer(\n",
        "        accelerator=\"gpu\",\n",
        "        devices=[0],\n",
        "        max_epochs=config.get(\"max_epochs\"),\n",
        "        accumulate_grad_batches=config.get(\"accumulate_grad_batches\"), # automatic opt \n",
        "        check_val_every_n_epoch=config.get(\"check_val_every_n_epoch\"),\n",
        "        gradient_clip_val=config.get(\"gradient_clip_val\"), # automatic opt \n",
        "        precision=\"16-mixed\", # 16\n",
        "        limit_val_batches=10,\n",
        "        num_sanity_val_steps=0,\n",
        "        val_check_interval=config.get(\"val_check_interval\"),  # % of an epoch\n",
        "        gradient_clip_algorithm=\"norm\",        #    \n",
        "\n",
        ")\n",
        "\n",
        "trainer.fit(model_module)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uT-no1yCN_7R"
      },
      "outputs": [],
      "source": [
        "torch.save(model_module.projection_module.state_dict(), \"/content/drive/MyDrive/model/llama3_multi.bin\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qjyqALqs_f_d"
      },
      "outputs": [],
      "source": [
        "model_module.save_pretrained(\"/content/drive/MyDrive/model/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bDzSPOcaH1g9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOacjwzCYU9B"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "CqcpNc2S9nnY",
        "1NsMV6Gi9slU",
        "qQcqZWt-9uLr",
        "rsPzVKJobdPb",
        "BYzWf8l5baD3"
      ],
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "00c33153cc204b65b8c02e4afd29ef22": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9409562fc76643a793bee60733b6bd4d",
              "IPY_MODEL_ac738f5c4afc481792d43f8cf03c22b1",
              "IPY_MODEL_733ba221c6b8404aa7c5c2130d8a91ab"
            ],
            "layout": "IPY_MODEL_dd71a55c80284e4a990803308bd9d199"
          }
        },
        "1a511570332d40fe9f2558f6d02d511d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b9960ae5fba4aa7b0ede4a6efb7de9a",
            "placeholder": "",
            "style": "IPY_MODEL_70510c2d0991472e96e8c28ac3759d09",
            "value": "10/10[03:07&lt;00:00,0.05it/s]"
          }
        },
        "1f8e8c194fb74a30acdc4604d22ff8f5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22107ffbfff74c2ba3d7a5646f7bb030": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3788356e345241ee9c6a56cd441eb161": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3a12a3b59bf54b029e06d0636640c195": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40531b020533449ba8eb5533b1025cf4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": "100%"
          }
        },
        "4eda4de0ffb4481899f5e68f6e4708d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5b9960ae5fba4aa7b0ede4a6efb7de9a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70510c2d0991472e96e8c28ac3759d09": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "733ba221c6b8404aa7c5c2130d8a91ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1a46bfccb0342749ff400cf4a27eff0",
            "placeholder": "",
            "style": "IPY_MODEL_af91d16b2ec248a3a4f01ba40ce59353",
            "value": "101100/238150[20:23:10&lt;27:38:07,1.38it/s,v_num=1,train_loss=0.0985,val_edit_distance=0.874]"
          }
        },
        "7aa7280af98a413d9de2bbcc4b7a8689": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_89bd3040bf61421f8389758d9259410b",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4eda4de0ffb4481899f5e68f6e4708d2",
            "value": 10
          }
        },
        "89bd3040bf61421f8389758d9259410b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90a3d8c43de34d4097c3ab933925af94": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f8e8c194fb74a30acdc4604d22ff8f5",
            "placeholder": "",
            "style": "IPY_MODEL_3788356e345241ee9c6a56cd441eb161",
            "value": "ValidationDataLoader0:100%"
          }
        },
        "9409562fc76643a793bee60733b6bd4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a12a3b59bf54b029e06d0636640c195",
            "placeholder": "",
            "style": "IPY_MODEL_c9530fb121c64b53adf918fcfc1536e0",
            "value": "Epoch0:38%"
          }
        },
        "9c97968a955f4c96b378cd71d2c1a981": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a1a46bfccb0342749ff400cf4a27eff0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac738f5c4afc481792d43f8cf03c22b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_22107ffbfff74c2ba3d7a5646f7bb030",
            "max": 238150,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9c97968a955f4c96b378cd71d2c1a981",
            "value": 101100
          }
        },
        "af91d16b2ec248a3a4f01ba40ce59353": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c67add08d6b7491a8c0863218a0b861b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_90a3d8c43de34d4097c3ab933925af94",
              "IPY_MODEL_7aa7280af98a413d9de2bbcc4b7a8689",
              "IPY_MODEL_1a511570332d40fe9f2558f6d02d511d"
            ],
            "layout": "IPY_MODEL_40531b020533449ba8eb5533b1025cf4"
          }
        },
        "c9530fb121c64b53adf918fcfc1536e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dd71a55c80284e4a990803308bd9d199": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
